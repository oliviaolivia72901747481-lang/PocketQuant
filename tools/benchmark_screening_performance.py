#!/usr/bin/env python3
"""
ç­›é€‰æµç¨‹æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

éªŒè¯å®Œæ•´ç­›é€‰æµç¨‹è€—æ—¶ â‰¤ 30åˆ†é’Ÿçš„æ€§èƒ½ç›®æ ‡

ä½¿ç”¨æ–¹æ³•:
    python tools/benchmark_screening_performance.py [--mock] [--target-minutes N]

å‚æ•°:
    --mock: ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    --target-minutes N: è®¾ç½®ç›®æ ‡è€—æ—¶ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤30

Requirements: æŠ€æœ¯çº¦æŸ - ç­›é€‰è¿‡ç¨‹åº”åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
"""

import sys
import os
import argparse
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tests.test_screening_performance import (
    ScreeningPerformanceTester,
    ScreeningPerformanceReport
)


def print_banner():
    """æ‰“å°æ¨ªå¹…"""
    print("=" * 70)
    print("  ç§‘æŠ€è‚¡æ± ç­›é€‰æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("  Tech Stock Pool Screening Performance Benchmark")
    print("=" * 70)


def print_progress(stage: str, elapsed: float, total_target: float):
    """æ‰“å°è¿›åº¦"""
    progress = min(100, elapsed / total_target * 100)
    bar_length = 40
    filled = int(bar_length * progress / 100)
    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
    print(f"\r  [{bar}] {progress:.1f}% - {stage}", end="", flush=True)


def run_benchmark(use_mock: bool = False, target_minutes: float = 30.0) -> bool:
    """
    è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    
    Args:
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        target_minutes: ç›®æ ‡è€—æ—¶ï¼ˆåˆ†é’Ÿï¼‰
    
    Returns:
        bool: æµ‹è¯•æ˜¯å¦é€šè¿‡
    """
    print_banner()
    print(f"\nğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ ç›®æ ‡è€—æ—¶: â‰¤ {target_minutes:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“Š æµ‹è¯•æ¨¡å¼: {'æ¨¡æ‹Ÿæ•°æ®' if use_mock else 'çœŸå®æ•°æ®'}")
    print("-" * 70)
    
    target_seconds = target_minutes * 60
    tester = ScreeningPerformanceTester(target_duration_seconds=target_seconds)
    
    print("\nğŸš€ å¼€å§‹ç­›é€‰æµç¨‹...")
    start_time = time.time()
    
    try:
        report = tester.run_full_screening_test(use_mock=use_mock)
        
        # æ‰“å°è¯¦ç»†æŠ¥å‘Š
        print("\n" + report.generate_report())
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_path = "tests/SCREENING_PERFORMANCE_REPORT.md"
        save_report(report, report_path)
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print("\n" + "=" * 70)
        if report.passed:
            print("âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡!")
            print(f"   å®é™…è€—æ—¶: {report.total_duration_minutes:.2f} åˆ†é’Ÿ")
            print(f"   ç›®æ ‡è€—æ—¶: {target_minutes:.1f} åˆ†é’Ÿ")
            margin = target_minutes - report.total_duration_minutes
            print(f"   ä½™é‡: {margin:.2f} åˆ†é’Ÿ ({margin/target_minutes*100:.1f}%)")
        else:
            print("âŒ æ€§èƒ½æµ‹è¯•æœªé€šè¿‡!")
            print(f"   å®é™…è€—æ—¶: {report.total_duration_minutes:.2f} åˆ†é’Ÿ")
            print(f"   ç›®æ ‡è€—æ—¶: {target_minutes:.1f} åˆ†é’Ÿ")
            excess = report.total_duration_minutes - target_minutes
            print(f"   è¶…å‡º: {excess:.2f} åˆ†é’Ÿ")
        print("=" * 70)
        
        return report.passed
        
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\n\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print(f"   å·²è€—æ—¶: {elapsed/60:.2f} åˆ†é’Ÿ")
        return False


def save_report(report: ScreeningPerformanceReport, path: str):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    with open(path, 'w', encoding='utf-8') as f:
        f.write("# ç­›é€‰æµç¨‹æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## æµ‹è¯•ç»“æœ\n\n")
        f.write(f"- **çŠ¶æ€**: {'âœ… é€šè¿‡' if report.passed else 'âŒ æœªé€šè¿‡'}\n")
        f.write(f"- **ç›®æ ‡è€—æ—¶**: â‰¤ {report.target_duration_minutes:.1f} åˆ†é’Ÿ\n")
        f.write(f"- **å®é™…è€—æ—¶**: {report.total_duration_minutes:.2f} åˆ†é’Ÿ\n\n")
        
        f.write("## å„é˜¶æ®µè€—æ—¶\n\n")
        f.write("| é˜¶æ®µ | è€—æ—¶(ç§’) | è®°å½•æ•° | çŠ¶æ€ |\n")
        f.write("|------|----------|--------|------|\n")
        for result in report.stage_results:
            status = "âœ…" if result.passed else "âŒ"
            f.write(f"| {result.stage} | {result.duration_seconds:.2f} | {result.records_processed} | {status} |\n")
        
        f.write("\n## è¯¦ç»†æŠ¥å‘Š\n\n")
        f.write("```\n")
        f.write(report.generate_report())
        f.write("\n```\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç§‘æŠ€è‚¡æ± ç­›é€‰æ€§èƒ½åŸºå‡†æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # è¿è¡ŒçœŸå®æ•°æ®æµ‹è¯•ï¼ˆé»˜è®¤30åˆ†é’Ÿç›®æ ‡ï¼‰
    python tools/benchmark_screening_performance.py
    
    # è¿è¡Œæ¨¡æ‹Ÿæ•°æ®å¿«é€Ÿæµ‹è¯•
    python tools/benchmark_screening_performance.py --mock
    
    # è®¾ç½®è‡ªå®šä¹‰ç›®æ ‡è€—æ—¶
    python tools/benchmark_screening_performance.py --target-minutes 20
        """
    )
    
    parser.add_argument(
        '--mock',
        action='store_true',
        help='ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•'
    )
    
    parser.add_argument(
        '--target-minutes',
        type=float,
        default=30.0,
        help='ç›®æ ‡è€—æ—¶ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤30'
    )
    
    args = parser.parse_args()
    
    success = run_benchmark(
        use_mock=args.mock,
        target_minutes=args.target_minutes
    )
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
